from typing import List, Any, Tuple, Union, Optional

import pandas as pd

from .constants import id_pattern, n_pattern, PLACEHOLDERS, span_pattern, TEXT_COL, multi_pattern, PARA_COL


def column_list(base_col, text_col):
    """Make column list for dataframe based upon base column name."""
    return [
        "file",
        "ID",
        text_col,
        id_pattern(base_col),
        base_col,
        span_pattern(base_col),
        n_pattern(base_col)
    ]


def increment_ids(tuple_list, increment):
    return [(idx + increment, e) for idx, e in tuple_list]


def add_id(elements: List[Any]) -> List[Tuple[int, Any]]:
    """
    Add an index to each element in the list. If the elements are tuples, the
    index is included as the first element in the tuple. Otherwise, returns a
    list of tuples where the first element is the index and the second element
    is the corresponding element from the input list.

    Args:
        elements (List[Any]): A list of elements of any type.
    Returns:
        List[Tuple[int, Any]]: A list of tuples where each tuple contains an
        index (int) and the corresponding element from the input list.
    """
    if all(isinstance(e, tuple) for e in elements):
        return [(idx, *e) for idx, e in enumerate(elements)]
    else:
        return list(enumerate(elements))


def clean_placeholders(
        data: Union[pd.DataFrame, List[str], List[tuple]],
        column: Optional[str] = None,
        placeholders: List[str] = PLACEHOLDERS)\
        -> Union[pd.DataFrame, List[str], List[tuple]]:
    """
    Remove placeholders from text_unit data. If data is a pandas dataframe, a
    column must be specified. If the dataframe contains a column with segment
    counts of the format generated by the TextSplitter class, the counts will
    be recalculated after removing placeholders.
    """
    if isinstance(data, pd.DataFrame):
        if column is None:
            raise ValueError("Column must be specified for pandas dataframes.")

        output_df = data[~data[column].isin(placeholders)]

        # recount segments
        if f"{column}s_n" in output_df.columns:
            output_df[f"{column}s_n"] = (output_df.groupby("file")["file"]
                                         .transform("size"))

        output_df = output_df.reset_index(drop=True)

        return output_df

    elif isinstance(data, list):
        return [p for p in data if p not in placeholders]

    elif isinstance(data, list) and isinstance(data[0], tuple):
        return [p for p in data if p[1] not in placeholders]

    else:
        raise ValueError("Data must be a pandas dataframe, list of strings or "
                         "list of tuples.")


def find_substring_indices(text, substrings):
    indices = []
    start = 0

    for substring in substrings:
        start = text.find(substring, start)
        if start == -1:
            raise ValueError(f"Substring '{substring}' not found in text.")
        end = start + len(substring)
        indices.append((start, end))
        start = end  # Move start to the end of the current substring for the next search

    return indices

def uniform_depth(obj: Any) -> int:
    """Return the depth of an object of nested lists. "Depth" signifies how many
    times you can uniformly peel away layers of lists before hitting a non-list
    element that can't be peeled any further. If it's not a list at all, the
    depth is 0.

    Examples:
    - [['foo'], ['bar']] -> 2
      (layer #1 is a list of lists, layer #2 is a list of strings, so we stop)
    - [['foo'], 'bar'] -> 1
      (layer #1: not all subelements are lists, so we stop immediately)
    - [] -> 1
      (layer #1 is a list, but it’s empty; we can’t go deeper)
    - 'foo' -> 0
      (not even a list)
    """
    depth = 0
    while isinstance(obj, list):
        depth += 1
        if not obj:  # empty list
            break
        # Check if all subelements are lists, then peel away list layer by
        # flattening them all
        if all(isinstance(sub, list) for sub in obj):
            obj = [item for sub in obj for item in sub]
        else:
            break
    return depth


def cast_to_df(
    input_df: pd.DataFrame,
    segments: List[Union[str, Tuple[Tuple[int, int], str]]],
    base_column: str,
    text_column: str = TEXT_COL,
    include_span: bool = False,
    drop_text: bool = True,
    mathematical_ids: bool = False
    ) -> pd.DataFrame:
    """
    From a list of segments and a dataframe with original text data, create a
    new segment dataframe with one row per segment.

    Args:
        input_df (pd.DataFrame): DataFrame with original text data
        segments (List[Union[str, Tuple[Tuple[int, int], str]]]): List of
            segments as strings or tuples including span information
        base_column (str): Base column name for segment data
        text_column (str): Column name with original text data
        include_span (bool): Whether to include span information in output
        drop_text (bool): Whether to drop the original text column
        mathematical_ids (bool): Whether to increment segment IDs by 1 to avoid 0

    Returns:
        pd.DataFrame: DataFrame with segment data as rows
    """
    multi_column = multi_pattern(base_column)
    id_column = id_pattern(base_column)
    span_column = span_pattern(base_column)
    n_column = n_pattern(base_column)

    df = input_df.copy()

    df[multi_column] = pd.Series(segments)
    df = df.explode(multi_column).reset_index(drop=True)

    # unpack segment data into separate columns
    segment_df = pd.DataFrame(df[multi_column].tolist())
    if include_span:
        df[[id_column, span_column, base_column]] = segment_df
    else:
        df[[id_column, base_column]] = segment_df

    # count segments per text
    df[n_column] = df.groupby(text_column)[text_column].transform("size")

    if mathematical_ids:
        df[id_column] = df[id_column].map(lambda x: x + 1)

    # keep only desired columns for output dataframe
    columns = [c for c in column_list(base_column, text_column) if c in df.columns]

    if drop_text:
        columns.remove(text_column)

    return df[columns]
